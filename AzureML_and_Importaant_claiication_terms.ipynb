{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AzureML and Importaant claiication terms.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOy/y2tw/HjcRqN4epYbhA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagu12/all-projects/blob/master/AzureML_and_Importaant_claiication_terms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBksCgQuFUqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#WORKFLOW OF AZURE ML\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-mQ-PZ_GJue",
        "colab_type": "text"
      },
      "source": [
        "GET THE DATA\n",
        "PREPARE THE DATA\n",
        "FEATURE SELECTION\n",
        "CHOOSE AND APPLY LEARNING ALGIORITHMS\n",
        "TRAIN AND EVALUATE THE MODEL\n",
        "\n",
        "Azure ML provides variety of ways such as filter based selection, Fisher LDA as well as Permutation Feature Importance. Within the filter based feature selection, it does provide us variety of options such as Pearson correlation, Chi Squared and so on.\n",
        "\n",
        "https://studio.azureml.net/\n",
        "\n",
        "https://notebooks.azure.com/sagu7065/projects/first\n",
        "\n",
        "unpack zip dataset can be used to unzip a file in the cloud\n",
        "\n",
        "Importing data from external sources can be performed in azure by using import data\n",
        "\n",
        "In azure user cache helps in storing the actions performed with the dataset rather than continuously updating the same manually.\n",
        "\n",
        "Merging two or more datasets is carried out by add column \n",
        "\n",
        "https://api.covid19india.org/\n",
        "\n",
        "To remove duplicate rows use the command remove duplicate rows\n",
        "\n",
        "To select specific columns in a dataset use the command select columns in dataset\n",
        "\n",
        "Using SQL transformation categorization of data is possible use the apply sql transformation\n",
        "\n",
        "To change the data type like string to float and vice versa use edit meta data command\n",
        "\n",
        "To clean missing data use clean missing data\n",
        "\n",
        "To partition the dataset into training and testing dataset use the command partition and sample module\n",
        "\n",
        "Random seed in partition means the division of data into specific values and if set to 0 then it will always tchange the data into random samples otherwise use 123 to fix the changed data permanently.\n",
        "\n",
        "Stratified split for sampling helps in creating a balance between the datasets we have selected for testing so should always be put true.\n",
        "\n",
        "split command helps in splitting test and train data.\n",
        "\n",
        "\n",
        "Hyperparameters are the knobs that data scientist use to get the optimum results for better accuracy\n",
        "\n",
        "Create train mode---> Single parameter----> Provide specific set of values\n",
        "Parameter range----> specify multiple values and get the optimum set for given configuration\n",
        "\n",
        "Optimization Tolerance-------> Threshold value to stop the model iterations on trained dataset.\n",
        "\n",
        "Minimize the cost of wrong prediction or errors using a cost function.\n",
        "\n",
        "Infinite or endless loop so optimization tolerance to know when to stop.\n",
        "\n",
        "Memory Size for L-BFGS-------> Amount of memory to use for next steps and direction.\n",
        "\n",
        "Unknown categorical level-------> Matches the unknown categories in the test data with the training data or vice versa\n",
        "\n",
        "L2(Ridge)------> shrinks all the coefficient by the same propotions but eliminates none.\n",
        "\n",
        "L1(Lasso)-------> can shrink some coefficients to zero performing variable selection.\n",
        "\n",
        "Both L1 and L2 prevents overfitting by shrinking or imposing a penalty on the coefficients.\n",
        "\n",
        "With L2 you tend to end up with many small weights, while with L1, you tend to end up with larger weights but more zeroes.\n",
        "\n",
        "To find the accuracy use the following formula---> true positives + true negatives / total sum of true and false positives and negatives of actual values.\n",
        "\n",
        "Precision is calculated as proportion of correct positive results / all predictive positive results.\n",
        "\n",
        "Recall is the proportion of positive cases or no. of positive predicted cases / total no. of actual positive cases\n",
        "\n",
        "\n",
        "F1 score is defined as the weighted average of the harmonic mean of precision and recall.\n",
        "\n",
        "Mathematically \n",
        "f1 score is represented as =\n",
        "2 * precision * recall / (precision + recall)\n",
        "\n",
        "AUC- Area under the curve\n",
        "\n",
        "ROC- Receiver Operating Characteristics\n",
        "\n",
        "True Positive rate-   True Positive / True Positive + False Negative\n",
        "\n",
        "False Positive rate-  False Positive / False Positive + True Negative\n",
        "\n",
        "Basically provides a number that lets you compare models of different types.\n",
        "\n",
        "Ranking randomly chosen positive observationn higher than randomly chosen negative observations for multiple threshold values. So ROC curve visualises all possible thresholds.\n",
        "\n",
        "https://www.youtube.com/watch?v=lZp-qcrQdCQ\n",
        "\n",
        "https://www.youtube.com/watch?v=7PWgx16kH8s\n",
        "\n",
        "#To summarize, large model weights can lead to overfitting, which leads to poor prediction accuracy. Regularization limits the magnitude of model weights by adding a penalty for weights to the model error function. L1 regularization uses the sum of the absolute values of the weights. L2 regularization uses the sum of the squared values of the weights.\n",
        "\n",
        "#Why do we use root mean square error?\n",
        "They can be positive or negative as the predicted value under or over estimates the actual value. Squaring the residuals, averaging the squares, and taking the square root gives us the r.m.s error. You then use the r.m.s. error as a measure of the spread of the y values about the predicted y value.\n",
        "\n",
        "#Based on a rule of thumb, it can be said that RMSE values between 0.2 and 0.5 shows that the model can relatively predict the data accurately. In addition, Adjusted R-squared more than 0.75 is a very good value for showing the accuracy. In some cases, Adjusted R-squared of 0.4 or more is acceptable as well.\n",
        "\n",
        "https://docs.microsoft.com/en-us/archive/msdn-magazine/2015/february/test-run-l1-and-l2-regularization-for-machine-learning\n",
        "\n",
        "https://www.youtube.com/watch?v=ksvJDLdc9eA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c64IFtQPvk7F",
        "colab_type": "code",
        "outputId": "35bd2308-e0e4-4922-bb08-f8af31ded57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "data = {\n",
        "        \"Inputs\": {\n",
        "                \"input1\":\n",
        "                [\n",
        "                    {\n",
        "                            'Loan_ID': \"LP001002\",   \n",
        "                            'Gender': \"Male\",   \n",
        "                            'Married': \"No\",   \n",
        "                            'Dependents': \"0\",   \n",
        "                            'Education': \"Graduate\",   \n",
        "                            'Self_Employed': \"No\",   \n",
        "                            'ApplicantIncome': \"5849\",   \n",
        "                            'CoapplicantIncome': \"0\",   \n",
        "                            'LoanAmount': \"1\",   \n",
        "                            'Loan_Amount_Term': \"360\",   \n",
        "                            'Credit_History': \"1\",   \n",
        "                            'Property_Area': \"Urban\",   \n",
        "                            'Loan_Status': \"Y\",   \n",
        "                    }\n",
        "                ],\n",
        "        },\n",
        "    \"GlobalParameters\":  {\n",
        "    }\n",
        "}\n",
        "\n",
        "body = str.encode(json.dumps(data))\n",
        "\n",
        "url = 'https://ussouthcentral.services.azureml.net/workspaces/63463826024a4f6ca252fa89d5e1f226/services/8805dbc87c654007a2e4d4f6ae4f188f/execute?api-version=2.0&format=swagger'\n",
        "api_key = 'krzdNytiB1EiXQkcxMg8EtEZMQ1eYmOERy8oC+9braZnAk6jNazQHt+bt4HmA6ZfTv7lYxfKbJryT1vu8oJfAg==' # Replace this with the API key for the web service\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
        "\n",
        "req = urllib.request.Request(url, body, headers)\n",
        "\n",
        "try:\n",
        "    response = urllib.request.urlopen(req)\n",
        "\n",
        "    result = response.read()\n",
        "    print(result)\n",
        "except urllib.error.HTTPError as error:\n",
        "    print(\"The request failed with status code: \" + str(error.code))\n",
        "\n",
        "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
        "    print(error.info())\n",
        "    print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'{\"Results\":{\"output1\":[{\"Gender\":\"Male\",\"Married\":\"No\",\"Dependents\":\"0\",\"Education\":\"Graduate\",\"Self_Employed\":\"No\",\"ApplicantIncome\":\"5849\",\"CoapplicantIncome\":\"0\",\"LoanAmount\":\"1\",\"Loan_Amount_Term\":\"360\",\"Credit_History\":\"1\",\"Property_Area\":\"Urban\",\"Loan_Status\":\"Y\",\"Scored Labels\":\"Y\",\"Scored Probabilities\":\"0.778199195861816\"}]}}'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}