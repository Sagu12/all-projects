{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plagiarism_tool.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcbA56btOAM1CI1icfnmlA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagu12/all-projects/blob/master/plagiarism_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU57YWZQuciw"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eaTvex_AFNu"
      },
      "source": [
        "f=open('demo.txt',\"r\")\r\n",
        "orig=f.read().replace(\"\\n\",\" \")\r\n",
        "f.close()\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LKTizm2BLUs"
      },
      "source": [
        "f2=open('demo1.txt',\"r\")\r\n",
        "plag= f2.read().replace(\"\\n\",\" \")\r\n",
        "f2.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a2F33BuFBa45",
        "outputId": "b34096c3-8a11-431a-87ae-84336d234dbd"
      },
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CSX1HfouBfoi",
        "outputId": "8198814d-36b1-4a52-a7b3-472b5bcc8575"
      },
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo4A5qvLNrLG"
      },
      "source": [
        "## Using Jaccard similarity and Contaminant measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYeXLXlCBXa4"
      },
      "source": [
        "#word tokenisation\r\n",
        "tokens_o=word_tokenize(orig)\r\n",
        "tokens_p=word_tokenize(plag)\r\n",
        "\r\n",
        "#lowerCase\r\n",
        "tokens_o = [token.lower() for token in tokens_o]\r\n",
        "tokens_p = [token.lower() for token in tokens_p]\r\n",
        "\r\n",
        "#stop word removal\r\n",
        "#punctuation removal\r\n",
        "stop_words=set(stopwords.words('english'))\r\n",
        "punctuations=['\"','.','(',')',',','?',';',':',\"''\",'``']\r\n",
        "filtered_tokens_o = [w for w in tokens_o if not w in stop_words and not w in punctuations]\r\n",
        "filtered_tokens_p = [w for w in tokens_p if not w in stop_words and not w in punctuations]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UlB2CZ2XBkZ_",
        "outputId": "84b6cdda-ec6a-42c3-87e5-516f464fb2ed"
      },
      "source": [
        "#Trigram Similiarity measures\r\n",
        "trigrams_o=[]\r\n",
        "for i in range(len(tokens_o)-2):\r\n",
        "    t=(tokens_o[i],tokens_o[i+1],tokens_o[i+2])\r\n",
        "    trigrams_o.append(t)\r\n",
        "\r\n",
        "s=0\r\n",
        "trigrams_p=[]\r\n",
        "for i in range(len(tokens_p)-2):\r\n",
        "    t=(tokens_p[i],tokens_p[i+1],tokens_p[i+2])\r\n",
        "    trigrams_p.append(t)\r\n",
        "    if t in trigrams_o:\r\n",
        "        s+=1\r\n",
        "\r\n",
        "#jaccord coefficient = (S(o)^S(p))/(S(o) U S(p))\r\n",
        "J=s/(len(trigrams_o)+len(trigrams_p)) \r\n",
        "print(J)\r\n",
        "\r\n",
        "#containment measure\r\n",
        "C=s/len(trigrams_p)\r\n",
        "print(C)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3125\n",
            "0.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a1Idw0N6Kq"
      },
      "source": [
        "## Using LCS method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB76F-IyBkh_"
      },
      "source": [
        "#longest common subsequence\r\n",
        "#dynamic programming algorithm for finding lcs\r\n",
        "def lcs(l1,l2):\r\n",
        "    s1=word_tokenize(l1)\r\n",
        "    s2=word_tokenize(l2)\r\n",
        "    # storing the dp values \r\n",
        "    dp = [[None]*(len(s1)+1) for i in range(len(s2)+1)] \r\n",
        "  \r\n",
        "    for i in range(len(s2)+1): \r\n",
        "        for j in range(len(s1)+1): \r\n",
        "            if i == 0 or j == 0: \r\n",
        "                dp[i][j] = 0\r\n",
        "            elif s2[i-1] == s1[j-1]: \r\n",
        "                dp[i][j] = dp[i-1][j-1]+1\r\n",
        "            else: \r\n",
        "                dp[i][j] = max(dp[i-1][j] , dp[i][j-1]) \r\n",
        "    return dp[len(s2)][len(s1)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BRpoQnQkBkpJ",
        "outputId": "42983d4f-e1b3-462a-8f67-1571357905a5"
      },
      "source": [
        "sent_o=sent_tokenize(orig)\r\n",
        "sent_p=sent_tokenize(plag)\r\n",
        "\r\n",
        "#maximum length of LCS for a sentence in suspicious text\r\n",
        "max_lcs=0\r\n",
        "sum_lcs=0\r\n",
        "\r\n",
        "for i in sent_p:\r\n",
        "    for j in sent_o:\r\n",
        "        l=lcs(i,j)\r\n",
        "        max_lcs=max(max_lcs,l)\r\n",
        "    sum_lcs+=max_lcs\r\n",
        "    max_lcs=0\r\n",
        "\r\n",
        "score=sum_lcs/len(tokens_p)\r\n",
        "print(score)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}