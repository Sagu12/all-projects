{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOt7a6jrEqCw2TqzNDUky89",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagu12/all-projects/blob/master/text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iec5d2M_gsKi"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import string\r\n",
        "import requests\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_-L9Gnzg5jF"
      },
      "source": [
        "response = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "ZcfoFvg6g5qW",
        "outputId": "3f131ec6-dcf3-4bd9-ea86-2ee059741243"
      },
      "source": [
        "response.text[:1500]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is the 100th Etext file presented by Project Gutenberg, and\\nis presented in cooperation with World Library, Inc., from their\\nLibrary of the Future and Shakespeare CDROMS.  Project Gutenberg\\noften releases Etexts that are NOT placed in the Public Domain!!\\n\\nShakespeare\\n\\n*This Etext has certain copyright implications you should read!*\\n\\n<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\\nSHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\\nPROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\\nWITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\\nDISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\\nPERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\\nCOMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\\nSERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\\n\\n*Project Gutenberg is proud to cooperate with The World Library*\\nin the presentation of The Complete Works of William Shakespeare\\nfor your reading for education and entertainment.  HOWEVER, THIS\\nIS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY\\nOF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY\\nBE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!\\nTO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!\\n\\n\\n**Welcome To The World of Free Plain Vanilla Electronic Texts**\\n\\n**Etexts Readable By Both Humans and By Computers, Since 1971**\\n\\n*These Etexts Prepared By Hundreds of Volunteers and Donations*\\n\\nInforma'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6I25_FgVg5vU",
        "outputId": "3bca4e21-820e-499a-d11b-f3423854acc6"
      },
      "source": [
        "data = response.text.split('\\n')\r\n",
        "data[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is the 100th Etext file presented by Project Gutenberg, and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8803oQsiF-0"
      },
      "source": [
        "The actual text starts from the 253 line therefore we will be taking the data from 253 line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KNX9tcQsg50U",
        "outputId": "28291915-9459-4049-9561-154c6f4be275"
      },
      "source": [
        "data = data[253:]\r\n",
        "data[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  From fairest creatures we desire increase,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENymZNRkg55y",
        "outputId": "024f5303-d9d1-4ed8-b96d-5f0ac1c36224"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "unC6e-Isg59q",
        "outputId": "420903f8-d5b2-486b-a6a6-8d958bf7a53d"
      },
      "source": [
        "data = \" \".join(data)\r\n",
        "data[:1000]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"  From fairest creatures we desire increase,   That thereby beauty's rose might never die,   But as the riper should by time decease,   His tender heir might bear his memory:   But thou contracted to thine own bright eyes,   Feed'st thy light's flame with self-substantial fuel,   Making a famine where abundance lies,   Thy self thy foe, to thy sweet self too cruel:   Thou that art now the world's fresh ornament,   And only herald to the gaudy spring,   Within thine own bud buriest thy content,   And tender churl mak'st waste in niggarding:     Pity the world, or else this glutton be,     To eat the world's due, by the grave and thee.                        2   When forty winters shall besiege thy brow,   And dig deep trenches in thy beauty's field,   Thy youth's proud livery so gazed on now,   Will be a tattered weed of small worth held:     Then being asked, where all thy beauty lies,   Where all the treasure of thy lusty days;   To say within thine own deep sunken eyes,   Were an all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz0ZX5BKg6Fb",
        "outputId": "3ffd14a0-014b-4e1b-a45e-d9437595a985"
      },
      "source": [
        "def clean_text(doc):\r\n",
        "  tokens = doc.split()\r\n",
        "  table = str.maketrans('', '', string.punctuation)\r\n",
        "  tokens = [w.translate(table) for w in tokens]\r\n",
        "  tokens = [word for word in tokens if word.isalpha()]\r\n",
        "  tokens = [word.lower() for word in tokens]\r\n",
        "  return tokens\r\n",
        "\r\n",
        "tokens = clean_text(data)\r\n",
        "print(tokens[:50])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautys', 'rose', 'might', 'never', 'die', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', 'feedst', 'thy', 'lights', 'flame', 'with', 'selfsubstantial', 'fuel', 'making', 'a', 'famine', 'where', 'abundance', 'lies', 'thy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZWuh9Xtg6Kk",
        "outputId": "65856e04-c6e1-436d-c20e-0ff4c5087f66"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "898199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULbVFInjjPw8"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-fD9NVvjFcZ",
        "outputId": "a4fc452c-cfa7-47c8-e59a-30d42705cfe7"
      },
      "source": [
        "len(np.unique(tokens))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q8lWGitjFmX",
        "outputId": "071bfff0-0df8-4999-f1af-8a844e2a94ef"
      },
      "source": [
        "length = 50 + 1\r\n",
        "lines = []\r\n",
        "\r\n",
        "for i in range(length, len(tokens)):\r\n",
        "  seq = tokens[i-length:i]\r\n",
        "  line = ' '.join(seq)\r\n",
        "  lines.append(line)\r\n",
        "  if i > 200000:\r\n",
        "    break\r\n",
        "\r\n",
        "print(len(lines))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "n6tHocWpjFvc",
        "outputId": "05ced9c0-69a9-4763-ef07-c105cd451529"
      },
      "source": [
        "lines[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a famine where abundance lies thy self'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jsg1jgjjjF5g",
        "outputId": "30ea5e3a-eeb7-4b02-d065-651727f97244"
      },
      "source": [
        "tokens[50]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'self'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf4mcfeljhAD"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(lines)\r\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yfCFuYcjhGx"
      },
      "source": [
        "sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p90kb1GxjhMR",
        "outputId": "37e9b7b2-23b6-4ea4-a4fb-720420d8d9ca"
      },
      "source": [
        "sequences = np.array(sequences)\r\n",
        "X, y = sequences[:, :-1], sequences[:,-1]\r\n",
        "X[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   47,  1408,  1264,    37,   451,  1406,     9,  2766,  1158,\n",
              "        1213,   171,   132,   269,    20,    24,     1,  4782,    87,\n",
              "          30,    98,  4781,    18,   715,  1263,   171,   211,    18,\n",
              "         829,    20,    27,  3807,     4,   214,   121,  1212,   153,\n",
              "       13004,    31,  2765,  1847,    16, 13003, 13002,   754,     7,\n",
              "        3806,    99,  2430,   466,    31])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUwYAG8SjhQx"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLTVW3GZj5WN"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfwV8gFAkUD6",
        "outputId": "8a6c8f3e-1cf5-4adf-c19f-39af56068e69"
      },
      "source": [
        "seq_length = X.shape[1]\r\n",
        "seq_length"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7eOpuOoj5b_"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\r\n",
        "model.add(LSTM(100, return_sequences=True))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5FqMOTFkD5e",
        "outputId": "524174fd-08b6-4e52-d754-40d11c7d90e9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 50)            650450    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 13009)             1313909   \n",
            "=================================================================\n",
            "Total params: 2,115,259\n",
            "Trainable params: 2,115,259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQwrTYmkD_W"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USNt-gkTkED1",
        "outputId": "77b70ad1-2028-4478-c2e4-295cc76e4b9d"
      },
      "source": [
        "model.fit(X, y, batch_size = 256, epochs = 20)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 435s 551ms/step - loss: 7.2045 - accuracy: 0.0279\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 440s 562ms/step - loss: 6.5312 - accuracy: 0.0417\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 414s 529ms/step - loss: 6.3331 - accuracy: 0.0564\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 409s 522ms/step - loss: 6.1436 - accuracy: 0.0706\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 394s 503ms/step - loss: 6.0099 - accuracy: 0.0794\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 386s 494ms/step - loss: 5.8659 - accuracy: 0.0901\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 378s 484ms/step - loss: 5.7666 - accuracy: 0.0961\n",
            "Epoch 8/20\n",
            "782/782 [==============================] - 382s 488ms/step - loss: 5.6839 - accuracy: 0.1017\n",
            "Epoch 9/20\n",
            "782/782 [==============================] - 375s 480ms/step - loss: 5.5877 - accuracy: 0.1046\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 378s 483ms/step - loss: 5.5226 - accuracy: 0.1058\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 377s 482ms/step - loss: 5.4686 - accuracy: 0.1084\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 380s 486ms/step - loss: 5.8530 - accuracy: 0.0868\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 385s 492ms/step - loss: 5.6007 - accuracy: 0.1013\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 377s 483ms/step - loss: 5.4671 - accuracy: 0.1052\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 379s 485ms/step - loss: 5.3717 - accuracy: 0.1089\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 372s 476ms/step - loss: 5.2858 - accuracy: 0.1118\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 378s 484ms/step - loss: 5.1950 - accuracy: 0.1148\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 368s 470ms/step - loss: 5.1294 - accuracy: 0.1156\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 369s 472ms/step - loss: 5.0520 - accuracy: 0.1179\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 364s 465ms/step - loss: 4.9765 - accuracy: 0.1222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1928c3ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "H4bRZ9rCkdv9",
        "outputId": "eabc7b6d-0ad1-49cd-f142-d915b303c406"
      },
      "source": [
        "seed_text=lines[1234]\r\n",
        "seed_text"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i behold the violet past prime and sable curls all silvered oer with white when lofty trees i see barren of leaves which erst from heat did canopy the herd and summers green all girded up in sheaves borne on the bier with white and bristly beard then of thy beauty'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DAtAInjkxdP"
      },
      "source": [
        "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\r\n",
        "  text = []\r\n",
        "\r\n",
        "  for _ in range(n_words):\r\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\r\n",
        "\r\n",
        "    y_predict = model.predict_classes(encoded)\r\n",
        "\r\n",
        "    predicted_word = ''\r\n",
        "    for word, index in tokenizer.word_index.items():\r\n",
        "      if index == y_predict:\r\n",
        "        predicted_word = word\r\n",
        "        break\r\n",
        "    seed_text = seed_text + ' ' + predicted_word\r\n",
        "    text.append(predicted_word)\r\n",
        "  return ' '.join(text)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "XY10GkqFkxo8",
        "outputId": "112765aa-cf21-4748-b9ad-e3fafcf9451d"
      },
      "source": [
        "  generate_text_seq(model, tokenizer, seq_length, seed_text, 100)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and freshest a man is a world and to be the world of the world of the world and th world and the world of the world of the world of the world and the roman of the king of the people of the king of the world of the king of the world of the world of the world of the king of the world of the king of the world of the king of the world of the king of the world of the world of the world of the world of the world of the world of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}